<!doctype html>
<html>
  <head>
    <title>Null by Null - Hyunsu Yu's Personal Dev Page</title>
    <meta charset="utf-8">
    <meta name="viewport" content="initial-scale=1">
    <link rel="stylesheet" href="../../element.css">
    <script src="../../element.js"></script>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/sunburst.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  </head>

  <body>
    <header>
        <h1>Body Tracking Using Webcam</h1>
        <strong>2D game development project dedicated to favorite streamers</strong>
        
        <br><br>
        <a href="../../index.html"><p><code style="text-decoration: none;color: white;">Back to Main Page</code></p></a>
    </header>

    <h2>Features</h2>
    <code>
        {
            "Features":[
                {
                    "Read real-time video data via webcam":"Get real-time video data for body tracking processing through a webcam on a desktop or laptop, or a camera on a mobile device"
                },
                {
                    "Extract the body's joint locations in real time from image data":"Using the API of the given library, data such as the location of human body joints are tracked and extracted in real time from image data obtained from webcams"
                }
            ]
        }  
    </code>

    <br>
    <h2>Abstract</h2>
    <p>The purpose of the Body Tracking project is to extract data such as the three-dimensional position and rotation angle of each part of the human body from video stream data</p>

    <br>
    <h2>Introduction</h2>
    <p>The project was carried out in two main ways</p>
    <p>The first method is using Microsoft's Azure Kinect DK, and the second method is to process video stream data received through a webcam on a web page using the body tracking model disclosed as an open source</p>

    <br>
    <h2>Body of Article</h2>
    <h3>Using Azure Kinect DK</h3>
    <article style="text-align: center;">
        <div>
            <img style="width: 60%;" class="Img" src="../../ImageSrcs\BodyTracking_1.png">
        </div>
        <div>
            <img style="width: 60%;" class="Img" src="https://docs.microsoft.com/en-us/azure/kinect-dk/media/concepts/joint-hierarchy.png">
        </div>
    </article>
    <br>
    <p>The way Azure Kinect DK is used is very simple, but you just need to call the equipment provided by Microsoft and the corresponding API</p>
    <p>The above example image is a program created in Windows Form, and the core file code is as follows</p>
    <code>
using System;
using System.Collections.Generic;
using System.Drawing;
using System.Threading.Tasks;
using System.Windows.Forms;
using System.Buffers;

using Microsoft.Azure.Kinect.Sensor;
using Microsoft.Azure.Kinect.BodyTracking;

using Image = Microsoft.Azure.Kinect.Sensor.Image;
using BitmapData = System.Drawing.Imaging.BitmapData;



namespace AzureKineticTest_1
{
    public partial class Form1 : Form
    {
        private Device kinectDevice;
        private DeviceConfiguration deviceConfiguration;

        ...
        

        public Form1()
        {
            InitializeComponent();

            InitKinect();

        }
        ~Form1()
        {
            kinectDevice.StopCameras();
            kinectDevice.Dispose();

            tracker.Dispose();

            isActive = false;
        }

        private async Task CalculateColor()
        {
            while (isActive)
            {
                using (Microsoft.Azure.Kinect.Sensor.Capture capture = await Task.Run(() => kinectDevice.GetCapture()).ConfigureAwait(true))
                {
                    unsafe
                    {
                        Image colorImage = capture.Color;

                        using (MemoryHandle pin = colorImage.Memory.Pin())
                        {
                            colorBitmap = new Bitmap(colorImage.WidthPixels, colorImage.HeightPixels, colorImage.StrideBytes, System.Drawing.Imaging.PixelFormat.Format32bppArgb, (IntPtr)pin.Pointer);
                        }

                        pictureBox_Color.Image = colorBitmap;
                    }

                    Update();
                }
            }

            return;
        }
        private async Task CalculateDepth()
        {
            while(isActive)
            {
                using (Capture capture = await Task.Run(() => kinectDevice.GetCapture()).ConfigureAwait(true))
                {
                    unsafe
                    {
                        Image depthImage = capture.Depth;

                        ushort[] depthArr = depthImage.GetPixels<ushort>().ToArray();
                        BitmapData bitmapData = depthBitmap.LockBits(new Rectangle(0, 0, depthBitmap.Width, depthBitmap.Height), System.Drawing.Imaging.ImageLockMode.WriteOnly, System.Drawing.Imaging.PixelFormat.Format32bppArgb);


                        byte* pixels = (byte*)bitmapData.Scan0;
                        
                        int depth = 0;
                        int tempIndex = 0;

                        for (int index = 0; index < depthArr.Length; index++)
                        {
                            depth = (int)(255 * (depthArr[index]) / 5000.0);

                            if (depth < 0 || depth > 255)
                            {
                                depth = 0;
                            }

                            tempIndex = index * 4;
                            pixels[tempIndex++] = (byte)depth;
                            pixels[tempIndex++] = (byte)depth;
                            pixels[tempIndex++] = (byte)depth;
                            pixels[tempIndex++] = 255;
                        }

                        depthBitmap.UnlockBits(bitmapData);


                        pictureBox_Depth.Image = depthBitmap;
                    }

                    Update();
                }
            }

            return;
        }
        private async Task CalcuateBodyTracking()
        {
            var calibration = kinectDevice.GetCalibration(deviceConfiguration.DepthMode, deviceConfiguration.ColorResolution);

            TrackerConfiguration trackerConfiguration = new TrackerConfiguration()
            {
                ProcessingMode = TrackerProcessingMode.Gpu,
                SensorOrientation = SensorOrientation.Default
            };

            tracker = Tracker.Create(
                calibration,
                new TrackerConfiguration
                {
                    SensorOrientation = SensorOrientation.Default,
                    ProcessingMode = TrackerProcessingMode.Gpu
                }
            );


            while (isActive)
            {
                using (Capture capture = await Task.Run(() => { return kinectDevice.GetCapture(); }))
                {
                    tracker.EnqueueCapture(capture);

                    using (Frame frame = tracker.PopResult())
                    using (Image color_image = frame.Capture.Color)
                    {
                        textBox_BodyTracking.Text = null;

                        jointData.Clear();

                        for (uint bodyIndex = 0; bodyIndex < frame.NumberOfBodies; bodyIndex++)
                        {
                            Skeleton skeleton = frame.GetBodySkeleton(bodyIndex);
                            uint id = frame.GetBodyId(bodyIndex);

                            if(bodyIndex >= 1)
                            {
                                textBox_BodyTracking.Text += Environment.NewLine + "------------------------------------------------------" + Environment.NewLine;
                            }

                            textBox_BodyTracking.Text += "Person Index : " + bodyIndex + Environment.NewLine;

                            jointData.Add(new Dictionary<JointId, Joint>());

                            for (int jointIndex = 0; jointIndex < (int)JointId.Count; jointIndex++)
                            {
                                Joint joint = skeleton.GetJoint(jointIndex);

                                textBox_BodyTracking.Text += "Joint Index : " + jointIndex + "   -   X : " + joint.Position.X + " / Y : " + joint.Position.Y + " / Z : " + joint.Position.Z + Environment.NewLine;

                                jointData[(int)bodyIndex].Add((JointId)jointIndex, joint);
                            }
                        }
                    }
                }
            }

            return;
        }

        ...

        private void InitKinect()
        {
            try
            {
                kinectDevice = Device.Open(0);

                deviceConfiguration = new DeviceConfiguration()
                {
                    ColorFormat = ImageFormat.ColorBGRA32,
                    ColorResolution = ColorResolution.R720p,
                    DepthMode = DepthMode.NFOV_2x2Binned,
                    CameraFPS = FPS.FPS15,
                    SynchronizedImagesOnly = true
                };
            }
            catch (AzureKinectException ex)
            {
                textBox_Error.Text += "1> Exception is occur during open kinect device" + Environment.NewLine + "1> Please check your device connection" + Environment.NewLine;
                textBox_Error.Text += ex.ToString() + Environment.NewLine;
            }
        }

        private void button_CameraCapture_Click(object sender, EventArgs e)
        {
            if(!isActive)
            {
                

                try
                {
                    kinectDevice.StartCameras(deviceConfiguration);
                }
                catch(AzureKinectException ex)
                {
                    textBox_Error.Text += "1> Exception is occur during start kinect device" + Environment.NewLine + "1> Please check your device connection" + Environment.NewLine;
                    textBox_Error.Text += ex.ToString() + Environment.NewLine;
                }


                isActive = true;

                InitBitMap();

                bool flag = true;
                if (checkBox_Color.Checked)
                {
                    flag = false;

                    CalculateColor();
                }
                if(checkBox_Depth.Checked)
                {
                    flag = false;

                    CalculateDepth();
                }
                if(checkBox_BodyTracking.Checked)
                {
                    flag = false;

                    CalcuateBodyTracking();

                    if(jointData == null)
                    {
                        jointData = new List<Dictionary<JointId, Joint>>();
                    }
                }
                
                if(flag)
                {
                    textBox_Error.Text += "=== No check box selected ===" + Environment.NewLine;
                    textBox_Error.Text += "[" + System.DateTime.Now.ToString("hh-mm-ss") + "] > Capture fail time" + Environment.NewLine;
                }
                else
                {
                    textBox_Error.Text += "=== Capture start ===" + Environment.NewLine;
                    textBox_Error.Text += "[" + System.DateTime.Now.ToString("hh-mm-ss") + "] > Capture start time" + Environment.NewLine;
                }
            }
            else
            {
                isActive = false;

                kinectDevice.StopCameras();
                tracker.Dispose();

                colorBitmap.Dispose();
                depthBitmap.Dispose();

                textBox_Error.Text += "=== Capture end ===" + Environment.NewLine;
                textBox_Error.Text += "[" + System.DateTime.Now.ToString("hh-mm-ss") + "] > Capture end time" + Environment.NewLine + Environment.NewLine;
            }
        }
        
        ...
    }
}
    </code>

    <article>
        <g-row>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://github.com/HyunsuYu/SimpleAzureKinectApp">
                    <div style="height: 50px; text-align: center;">
                        <h3>Example of a simple application using Azure Kinect DK</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="../../ImageSrcs\BodyTracking_1.png">
                    </div>
                </a>
            </g-col>
          </g-row>
    </article>

    <h3>Using Open Source ML Model and Webcam Video Stream Data</h3>
    <article style="text-align: center;">
        <div>
            <img style="width: 60%;" class="Img" src="../../ImageSrcs\BodyTracking_0.png">
        </div>
    </article>
    <br>
    <p>In this case, it was implemented by combining codes through web search, and consisted of simple html and javascript codes</p>
    <code>
        <!-- main.html -->
        <h2> align=center>Auto Video Stream to Still Image</h2>
                                        
        <video id="myVideo" width="400" height="300" style="border: 1px solid #ddd;"></video>
        <canvas id="myCanvas" width="160" height="140" style="border: 1px solid #ddd;"></canvas><br>
                                        
        <input type=button value="get Video" onclick="{getVideo()}">
        <input type=button value="get Pic" onclick="{takeSnapshot()}"><br>Take snapshot every
        <input type=number id="myInterval" value="3000"> milliseconds
        <input type=button value="Auto" onclick="{takeAuto()}">
                                        
        <script src="processing.js"></script>
                                        
        <script src="tf.min.js"></script>
        <script src="posenet.min.js"></script>
    </code>
    <code>
        // processing.js
        var myVideoStream = document.getElementById('myVideo')     // make it a global variable
        var myStoredInterval = 0

        function getVideo() {
            navigator.getMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
            navigator.getMedia({ video: true, audio: false },

            function (stream) {
                myVideoStream.srcObject = stream
                myVideoStream.play();
            },

            function (error) {
                alert('webcam not working');
            });
        }

        function takeSnapshot() {
            var myCanvasElement = document.getElementById('myCanvas');
            var myCTX = myCanvasElement.getContext('2d');
            myCTX.drawImage(myVideoStream, 0, 0, myCanvasElement.width, myCanvasElement.height);

            StartTracking()
        }

        function takeAuto() {
            takeSnapshot() // get snapshot right away then wait and repeat
            clearInterval(myStoredInterval)
            myStoredInterval = setInterval(function () {
                takeSnapshot()
            }, document.getElementById('myInterval').value);
        }

        function StartTracking() {
            var imageScaleFactor = 0.5;
            var outputStride = 16;
            var flipHorizontal = false;

            var imageElement = document.getElementById('myVideo');

            posenet.load().then(function (net) {
                return net.estimateSinglePose(imageElement, imageScaleFactor, flipHorizontal, outputStride)
            }).then(function (pose) {
                console.log(pose);
            })
        }
    </code>
    <p>The files required for ML are as follows</p>
    <article>
        <g-row>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://download.blog.naver.com/open/881d9420360302b69b7d122c15f488f05502fe76/ECCLlylrC79ZMallq_Izn6jSH801d4D924IL6pOYF8dtRdph1z4AqLvU6RHIJHeo4Mj7H9JOejHwNkaKhMWW2g/posenet.min.js">
                    <div style="height: 50px; text-align: center;">
                        <h3>posenet.min.js</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="..\..\ImageSrcs\folder.png">
                    </div>
                </a>
            </g-col>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://download.blog.naver.com/open/80159c2b380b0abe93751a241dfc80f85d0af69b/s2A1L5WL5wUMaO3v65SV1mP_04rUWEidJXyMNmGwGicLtq5XNyDnPZunsCd5Xv4mgEFIwzDrQkF2mh6r9wDQ3w/tf.min.js">
                    <div style="height: 50px; text-align: center;">
                        <h3>tf.min.js</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="..\..\ImageSrcs\folder.png">
                    </div>
                </a>
            </g-col>
          </g-row>
    </article>

    <br><br>
    <p>Related articles and demonstrations are as follows</p>
    <article>
        <g-row>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://blog.naver.com/lioie6478/223023653386">
                    <div style="height: 50px; text-align: center;">
                        <h3>My Original Blog Post Related to</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="https://blogpfthumb-phinf.pstatic.net/MjAyMTAzMDhfMjY1/MDAxNjE1MjExMDk2MzIz.Z7dS8pGG3U8kZ_d-FRrLtv1JZfQc8a0-XXLPI7DkFT0g.8xRClM1lFx2GkF1TLUyKKtSWlEyy6M1MK_XBEmH1SHQg.PNG.lioie6478/profileImage.png?type=w161">
                    </div>
                </a>
            </g-col>
            <g-col>
                <a style="text-decoration: none;color: black;" href="../../CompletedProjects/Project_BodyTracking/Demo/main.html">
                    <div style="height: 50px; text-align: center;">
                        <h3>Body Tracking Demo Page</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="../../ImageSrcs\BodyTracking_0.png">
                    </div>
                </a>
            </g-col>
          </g-row>
    </article>

    <br>
    <h2>Result</h2>
    <p>After testing the two methods, the pros and cons of each were clear</p>
    <p>First of all, the difficulty of development becomes easier when Azure Kinect DK is used, but the disadvantages can be pointed out as the fact that the equipment is not cheap and that the available platforms can be limited to some extent because the API provided is based on C#</p>
    <p>Next, in the case of a combination of ML models and webcams, the ML model currently tested was written in JavaScript, confirming the possibility of widespread use in the web environment and native, and by using webcams together, it could be easily utilized without spending money for additional equipment. However, since ML models are open-source based, it is unclear whether they are updated or not, and the quality of body tracking results can depend on various issues such as resolution of webcams</p>
        
    <br>
    <h2>References</h2>
    <code>
        {
            "References":[
                {
                    "[1] Azure Kinect DK documentation":"https://learn.microsoft.com/en-us/azure/kinect-dk/"
                },
                {
                    "[2] Real-time extraction of joint location from image data":"https://codepen.io/rocksetta/pen/BPbaxQ"
                },
                {
                    "[3] pose_estimation":"https://github.com/llSourcell/pose_estimation"
                }
            ]
        }
    </code>
    <article>
        <g-row>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://learn.microsoft.com/en-us/azure/kinect-dk/">
                    <div style="height: 50px; text-align: center;">
                        <h3>Azure Kinect DK documentation</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="https://learn.microsoft.com/en-us/azure/kinect-dk/media/concepts/concepts-coordinate-systems/coordinate-systems-gyroscope.png">
                    </div>
                </a>
            </g-col>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://codepen.io/rocksetta/pen/BPbaxQ">
                    <div style="height: 50px; text-align: center;">
                        <h3>Real-time extraction of joint location from image data</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <!-- <img style="width: 100%; height: 100%; object-fit: cover;" src="https://shared.akamai.steamstatic.com/store_item_assets/steam/apps/1289310/header.jpg?t=1620755466"> -->
                    </div>
                </a>
            </g-col>
            <g-col>
                <a style="text-decoration: none;color: black;" href="https://github.com/llSourcell/pose_estimation">
                    <div style="height: 50px; text-align: center;">
                        <h3>pose_estimation</h3>
                    </div>
                    <div style="height: 250px; width: 250px; overflow: hidden; margin:0 auto;">
                        <img style="width: 100%; height: 100%; object-fit: cover;" src="https://i.ytimg.com/an_webp/9KqNk5keyCc/mqdefault_6s.webp?du=3000&sqp=CI_Zl7MG&rs=AOn4CLC6QUyR_PbpbOAWPmXhFBnfQcHagg">
                    </div>
                </a>
            </g-col>
          </g-row>
    </article>

    <footer>
        <p>Hyunsu Yu, a developer who is interested in game development and interactive storytelling</p>
    </footer>
  </body>
</html>
